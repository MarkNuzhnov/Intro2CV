{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfa01e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN, OPTICS\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# matches we have to get to identify as match on image\n",
    "MIN_MATCH_COUNT = 5\n",
    "\n",
    "# supposed to be 1\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "\n",
    "# threshold\n",
    "threshold = 0.9\n",
    "\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "def close_1(img, points, h, w):\n",
    "    \"\"\"\n",
    "    checks if this is rectangle\n",
    "    \"\"\"\n",
    "    x_array = [points[i] for i in range(1, 8, 2)]\n",
    "    x_min, x_max = min(x_array), max(x_array)\n",
    "\n",
    "    y_array = [points[j] for j in range(0, 7, 2)]\n",
    "    y_min, y_max = min(y_array), max(y_array)\n",
    "\n",
    "    img2 = img[x_min:x_max, y_min:y_max]\n",
    "    h_img2, w_img2 = img2.shape\n",
    "    return np.isclose(h_img2 / h, w_img2 / w, 0.23)\n",
    "#------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "def find_and_polly(img, key_points_query, description_query, query, sift, flann):\n",
    "    \"\"\"\n",
    "    this function takes an image and query then uses sift to find key points\n",
    "    on img and query, then uses fillPoly to delete already found area\n",
    "    and gives coordinates of cropped area\n",
    "    can be done recursively with abusing of res (boolType)\n",
    "    \"\"\"\n",
    "    # finding key points and descriptors on img\n",
    "    key_points_img, descriptors_img = sift.detectAndCompute(img, None)\n",
    "\n",
    "    # finding matches with knn on img with desc on query    \n",
    "    matches = flann.knnMatch(description_query, descriptors_img, k=2)\n",
    "\n",
    "    # creating a list & add into it if distance satisfies conditions\n",
    "    dist = []\n",
    "    for i, (m, n) in enumerate(matches):\n",
    "        if m.distance < threshold * n.distance:\n",
    "            dist.append(m)\n",
    "\n",
    "    # list of distances\n",
    "    dst_pt = [key_points_img[m.trainIdx].pt for m in dist]\n",
    "\n",
    "    # labels via DBSCAN fitted on distances\n",
    "    labels = DBSCAN(eps=100, min_samples = 3).fit_predict(dst_pt)\n",
    "    #labels = OPTICS(max_eps=100).fit_predict(dst_pt)\n",
    "    \n",
    "    # let's create a dict with unique labels\n",
    "    uniq = {}   \n",
    "    for pos, a in enumerate(labels):\n",
    "        if not (a in uniq):\n",
    "            uniq[a] = 1\n",
    "        else:\n",
    "            uniq[a] +=1\n",
    "    #print(labels)\n",
    "\n",
    "    # max element\n",
    "    max_el = max(uniq, key=uniq.get)\n",
    "\n",
    "    # creating array of DMatch\n",
    "    d_match_array = []\n",
    "    for n, x in enumerate(labels): \n",
    "        if x == max_el:\n",
    "            d_match_array.append(dist[n])\n",
    "\n",
    "    # check if less than our min_match we return just img,\n",
    "    # else we get location of matched key points in both images\n",
    "    # use transformation matrix to transform the corners of query\n",
    "    # to corresponding points in img\n",
    "    if len(d_match_array) > MIN_MATCH_COUNT:\n",
    "        src_pts = np.float32([key_points_query[m.queryIdx].pt for m in d_match_array]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([key_points_img[m.trainIdx].pt for m in d_match_array]).reshape(-1, 1, 2)\n",
    "        \n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "        matched_mask = mask.ravel().tolist()\n",
    "        \n",
    "        h, w = query.shape[0], query.shape[1]\n",
    "        pts = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)\n",
    "        dst = cv2.perspectiveTransform(pts, M)\n",
    "        \n",
    "        pts_transformed = np.int32(dst).reshape(8).tolist()\n",
    "        close_1(img, pts_transformed, h, w)\n",
    "\n",
    "        if not close_1(img, pts_transformed, h, w):\n",
    "            return [False, img, [-1, -1, -1, -1]]\n",
    "\n",
    "        img2 = cv2.fillPoly(img, [np.int32(dst)], 255)    # fillPoly to fill the already found area\n",
    "        bbox = pts_transformed[:2] + pts_transformed[4:6]   \n",
    "\n",
    "        return [True, img2, bbox]\n",
    "    else:\n",
    "        return [False, img, [-1, -1, -1, -1]]\n",
    "#----------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#------------------------------------------------------------#\n",
    "def predict_image_1(img: np.ndarray, query: np.ndarray) -> list: \n",
    "    # convert to gray\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    query = cv2.cvtColor(query, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # creating a SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # finding key points and descriptors on query\n",
    "    key_points_query, description_query = sift.detectAndCompute(query, None)\n",
    "\n",
    "    # index_params & search_params\n",
    "    \n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=40)\n",
    "\n",
    "    # flann matcher\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    \n",
    "    # recursively use find_and_polly to find all matches on image and crop them until we don't have matches\n",
    "    img_true, new_img, bbox = find_and_polly(img, key_points_query, description_query, query, sift, flann)\n",
    "    list_of_bboxes = []\n",
    "    while img_true:\n",
    "        list_of_bboxes.append(bbox)\n",
    "        img_true, new_img, bbox = find_and_polly(new_img, key_points_query, description_query, query, sift, flann)\n",
    "\n",
    "    return list_of_bboxes\n",
    "#------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "224f578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "# matches we have to get to identify as match on image\n",
    "MIN_MATCH_COUNT = 7\n",
    "\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "threshold = 0.9\n",
    "\n",
    "\n",
    "def in_rect(points, h, w):\n",
    "    x, y = points[1:8:2], points[:7:2]\n",
    "    x_min, x_max = min(x), max(x)\n",
    "    y_min, y_max = min(y), max(y)\n",
    "\n",
    "    h_2, w_2 = x_max - x_min, y_max - y_min\n",
    "    return np.isclose(h_2 / h, w_2 / w, 0.2)\n",
    "\n",
    "\n",
    "def search(img, query):\n",
    "    sift = cv2.SIFT_create()\n",
    "    key_points_img, descriptors_img = sift.detectAndCompute(img, None)\n",
    "    key_points_query, description_query = sift.detectAndCompute(query, None)\n",
    "\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=40)\n",
    "\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(description_query, descriptors_img, k=2)\n",
    "\n",
    "    good = []\n",
    "    for i, (m, n) in enumerate(matches):\n",
    "        if m.distance < threshold * n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    dist = [key_points_img[m.trainIdx].pt for m in good]\n",
    "    labels = DBSCAN(eps=110, min_samples=3).fit_predict(dist)\n",
    "    labels_cnt = Counter(labels)\n",
    "    # print(labels_cnt)\n",
    "    most_frequent = max(labels_cnt, key=labels_cnt.get)\n",
    "    cluster = [good[n] for n, label in enumerate(labels) if label == most_frequent]\n",
    "    if len(cluster) > MIN_MATCH_COUNT:\n",
    "        src_pts = np.float32([key_points_query[m.queryIdx].pt for m in cluster]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([key_points_img[m.trainIdx].pt for m in cluster]).reshape(-1, 1, 2)\n",
    "\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        h, w = query.shape\n",
    "        pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "        dst = cv2.perspectiveTransform(pts, M)\n",
    "\n",
    "        pts_transformed = np.int32(dst).reshape(8).tolist()\n",
    "        if not in_rect(pts_transformed, h, w):\n",
    "            return False, img, None\n",
    "\n",
    "        img2 = cv2.fillPoly(img, [np.int32(dst)], 0)\n",
    "        box = pts_transformed[:2] + pts_transformed[4:6]\n",
    "\n",
    "        return True, img2, box\n",
    "    else:\n",
    "        return False, img, None\n",
    "\n",
    "\n",
    "def predict_image(img: np.ndarray, query: np.ndarray) -> list:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    query = cv2.cvtColor(query, cv2.COLOR_RGB2GRAY)\n",
    "    new_img = img.copy()\n",
    "\n",
    "    flag, boxes = True, []\n",
    "    while flag:\n",
    "        flag, new_img, box = search(new_img, query)\n",
    "        if box: boxes.append(box)\n",
    "\n",
    "    return boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c9d6ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1887796613905165,\n",
       "  0.46721563339233396,\n",
       "  0.08911378648546008,\n",
       "  0.10624017715454101),\n",
       " (0.18798156314425998,\n",
       "  0.6155833244323731,\n",
       "  0.07697185940212674,\n",
       "  0.10519037246704102),\n",
       " (0.15862456427680122,\n",
       "  0.7457643508911133,\n",
       "  0.09892429775661893,\n",
       "  0.11660776138305665),\n",
       " (0.1805374993218316,\n",
       "  0.3056957483291626,\n",
       "  0.08909077114529079,\n",
       "  0.10990607738494873),\n",
       " (0.2789219538370768,\n",
       "  0.4694417953491211,\n",
       "  0.08449501461452907,\n",
       "  0.10391902923583984)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_1 = cv2.imread('train/train_0.jpg')\n",
    "img_2 = cv2.imread('train/template_0_0.jpg')\n",
    "predict_image(img_1, img_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e67e43ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[135, 598, 200, 734],\n",
       " [134, 786, 190, 919],\n",
       " [129, 391, 194, 535],\n",
       " [116, 958, 187, 1112],\n",
       " [200, 600, 261, 735],\n",
       " [190, 952, 259, 1107],\n",
       " [117, 1159, 182, 1303],\n",
       " [185, 154, 251, 319],\n",
       " [196, 394, 256, 537],\n",
       " [194, 1158, 262, 1291]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image_1(img_1, img_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab5f2af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
